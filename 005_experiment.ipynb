{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9183228-0ba6-4af9-8430-649e28868253",
   "metadata": {
    "id": "JMXGlIvAwn30"
   },
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Detailing thinking\n",
    "\n",
    "In this case we will try to add tags and hoping that the model thinks before answering so it has a better response."
   ],
   "id": "5dd2a0022beefa14"
  },
  {
   "cell_type": "code",
   "id": "f5308d65",
   "metadata": {
    "height": 319,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-01T11:38:14.896493Z",
     "start_time": "2025-02-01T11:38:11.934257Z"
    }
   },
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "import helper.creden as cre\n",
    "client = OpenAI(api_key=cre.OPENAI_API_KEY)\n",
    "from helper.helper import parse_conversation, select_random_test, select_random_topics, get_completion_from_messages\n",
    "\n",
    "# widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "client = OpenAI(api_key=cre.OPENAI_API_KEY)\n",
    "model_selection=\"gpt-4o-mini\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T11:38:16.096197Z",
     "start_time": "2025-02-01T11:38:14.898358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_verification=[{\"role\": \"user\", \"content\": \"Are you working?\"}]\n",
    "result = get_completion_from_messages(messages = messages_verification, model = model_selection, temperature=0.8, max_tokens_values=4)\n",
    "result"
   ],
   "id": "abf423b94ad1bf00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, I'm here\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have confirmed the API is working, we will conduct an experiment.",
   "id": "70b78dea5a53fe33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T11:38:16.158611Z",
     "start_time": "2025-02-01T11:38:16.103759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tests_interviews = {\n",
    "                \"topics\": [\n",
    "                    {\n",
    "                    \"name\": \"Predictive Modeling\",\n",
    "                    \"example\": \"Predicting customer churn for a subscription service.\",\n",
    "                    \"problem\": \"Identifying which customers are likely to stop using the service and why.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Recommendation Systems\",\n",
    "                    \"example\": \"Suggesting products on an e-commerce platform.\",\n",
    "                    \"problem\": \"Providing personalized recommendations to improve user engagement and sales.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Natural Language Processing (NLP)\",\n",
    "                    \"example\": \"Sentiment analysis on customer reviews.\",\n",
    "                    \"problem\": \"Understanding customer sentiment to improve product offerings or service quality.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Time Series Analysis\",\n",
    "                    \"example\": \"Forecasting monthly sales for inventory planning.\",\n",
    "                    \"problem\": \"Predicting demand accurately to avoid stockouts or overstocking.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Anomaly Detection\",\n",
    "                    \"example\": \"Detecting fraudulent transactions in banking systems.\",\n",
    "                    \"problem\": \"Identifying unusual patterns or behaviors that indicate fraud.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Image and Video Analytics\",\n",
    "                    \"example\": \"Detecting defects in manufacturing products using image classification.\",\n",
    "                    \"problem\": \"Automating quality control to reduce manual inspection errors.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Customer Segmentation\",\n",
    "                    \"example\": \"Grouping customers by purchasing behavior.\",\n",
    "                    \"problem\": \"Creating targeted marketing campaigns to boost conversion rates.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Optimization Models\",\n",
    "                    \"example\": \"Optimizing delivery routes for a logistics company.\",\n",
    "                    \"problem\": \"Minimizing fuel costs and delivery time.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"A/B Testing\",\n",
    "                    \"example\": \"Testing two website layouts to improve user conversion.\",\n",
    "                    \"problem\": \"Determining which design leads to better engagement or sales.\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"Big Data Processing\",\n",
    "                    \"example\": \"Analyzing clickstream data from millions of users on a streaming platform.\",\n",
    "                    \"problem\": \"Deriving insights and recommendations from massive, unstructured datasets.\"\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "# select a random test\n",
    "random_topic = select_random_test(tests_interviews)\n",
    "random_topic\n",
    "\n",
    "\n",
    "data_science_topics = [\n",
    "    \"Data Preprocessing\", \"Exploratory Data Analysis (EDA)\", \"Feature Engineering\", \"Statistical Analysis\",\n",
    "    \"Probability Theory\", \"Supervised Learning\", \"Unsupervised Learning\", \"Model Evaluation Metrics\",\n",
    "    \"Cross-Validation Techniques\", \"Overfitting and Underfitting\", \"Bias-Variance Tradeoff\", \n",
    "    \"Ensemble Learning\", \"Neural Networks\", \"Natural Language Processing (NLP)\",\n",
    "    \"Time Series Analysis\", \"Spark\", \"Model Deployment and Monitoring\", \n",
    "    \"Reinforcement Learning\", \"Data Ethics and Privacy\", \"Cloud Computing for Data Science\"\n",
    "]\n",
    "\n",
    "selected_topics = select_random_topics(data_science_topics)\n",
    "selected_topics "
   ],
   "id": "e505a80c39576cc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model Deployment and Monitoring',\n",
       " 'Exploratory Data Analysis (EDA)',\n",
       " 'Probability Theory',\n",
       " 'Supervised Learning',\n",
       " 'Ensemble Learning']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "89d1f0e91d3fef34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompt generation",
   "id": "de87d507af361a44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T11:58:26.189041Z",
     "start_time": "2025-02-01T11:58:26.150625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_example = f\"\"\"\n",
    "When you reply, first plan how you should answer within <thinking> </thinking> XML tags. this is a space for you to write down relevant content and will be be showing to the used, for now.\n",
    "once you are done with the thinking you can output the answer of the thinking <answer> </answer> XML tags. Make sure your answer is detailed and specific  \n",
    "We are conducting a data science interview focused exclusively on \n",
    "data science topics. Avoid discussing unrelated subjects. Ensure\n",
    "a rigorous and structured interview process with the goal of \n",
    "evaluating the candidate's expertise in data science. \n",
    "for each well-answered question, and maintain high \n",
    "expectations as a data science professional with 5 years of\n",
    "experience. Look for key terminology and concepts relevant\n",
    "to the field.\n",
    "\n",
    "---------------\n",
    "Generated Knowledge Example Question: \n",
    "- We are working with a healthcare company to predict the likelihood of patients being readmitted within 30 days of discharge. The dataset includes patient demographics, medical history, and various other details related to their hospital stay. What steps would you take to build a model for this prediction task?\n",
    "- You’ve chosen a random forest model for predicting house prices. Could you explain what might happen if the model starts overfitting the data? What steps would you take to mitigate overfitting in this case, and how would you determine if the model is truly generalizing well?\n",
    "\n",
    "---------------\n",
    "<question>\n",
    "Steps to Follow:\n",
    "Introduction: Begin with a brief introduction of the company. Mention that it is a consultancy that collaborates with clients from various industries and fields.\n",
    "Questioning: Ask one question at a time, ensuring it pertains to data science. Move on to the next question only after receiving an answer. ask just 3 question in each interview as much\n",
    "Example Problem: Incorporate this field example {random_topic[\"example\"]} and this problem {random_topic[\"problem\"]} as a case study to keep the discussion grounded, also include this topics in the questions {selected_topics}.\n",
    "Dataset Creation: Design a simple dataset with 5 rows and 4–8 features, including a target variable, to use as part of the problem-solving exercise. Present this dataset in a table format.\n",
    "this its the example of the dataset\n",
    "Question Design: Formulate one thoughtful question at a time based on the dataset or case study. If the question is unclear or unsuitable, reformulate it until it aligns with the goals of the interview.\n",
    "Scoring: Assign a score from 0 to 9 for each response, where 9 represents expert-level knowledge. Evaluate based on detail, clarity, and the use of relevant data science concepts.\n",
    "Conclusion: Conclude by thanking the candidate for their time and providing an overall score based on their performance.\n",
    "Feedback: After completing all questions, identify areas where the interview process could be improved, providing specific points for enhancement.\n",
    "Final Output: Summarize the results in a JSON format, including the overall score and interview feedback.\n",
    "</question>\n",
    "---------------\n",
    "Interview Scoring System:\n",
    "Each question is scored from 0 to 9, where 9 reflects expert-level proficiency. Evaluate the answers based on:\n",
    "Clarity: Was the answer well-structured and easy to follow?\n",
    "Detail: Did the candidate demonstrate depth in their understanding of data science principles?\n",
    "Application of Concepts: Did the candidate incorporate relevant data science terminology and concepts (e.g., feature engineering, model selection, evaluation metrics)?\n",
    "Example: \"7/9\" or \"0/9\".\n",
    "\n",
    "---------------\n",
    "before ending the chat add a JSON using this format\n",
    "<Example>\n",
    "Extracted Data:\n",
    "[\n",
    "  \"candidate_name\": \"Jackeline\",\n",
    "  \"scores\": [\n",
    "    \"question_1\": 5,\n",
    "    \"question_2\": 6,\n",
    "    \"question_3\": 0\n",
    "  ],\n",
    "  \"overall_score\": 11,\n",
    "  \"feedback\": [\n",
    "    \"Depth of Knowledge: While you demonstrated some understanding of data science concepts, there is a need for deeper knowledge in model evaluation and data preprocessing techniques.\",\n",
    "    \"Specificity: Providing more detailed explanations and examples would enhance your responses.\",\n",
    "    \"Preparation: Familiarizing yourself with key evaluation metrics and their implications in classification problems would be beneficial.\"\n",
    "  ]\n",
    "]\n",
    "</Example>\n",
    "\n",
    "Reminder: Ensure the questions and evaluations are rigorous, leveraging your data science expertise to assess technical depth and problem-solving abilities.\n",
    "\n",
    "After printing the json just keep saying good bye in case the person wants to keep talking.\n",
    "\"\"\"\n",
    "\n",
    "context = [{'role':'system', 'content':prompt_example}]\n",
    "context"
   ],
   "id": "e59b56759644943c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\nWhen you reply, first plan how you should answer within <thinking> </thinking> XML tags. this is a space for you to write down relevant content and will be be showing to the used, for now.\\nonce you are done with the thinking you can output the answer of the thinking <answer> </answer> XML tags. Make sure your answer is detailed and specific  \\nWe are conducting a data science interview focused exclusively on \\ndata science topics. Avoid discussing unrelated subjects. Ensure\\na rigorous and structured interview process with the goal of \\nevaluating the candidate\\'s expertise in data science. \\nfor each well-answered question, and maintain high \\nexpectations as a data science professional with 5 years of\\nexperience. Look for key terminology and concepts relevant\\nto the field.\\n\\n---------------\\nGenerated Knowledge Example Question: \\n- We are working with a healthcare company to predict the likelihood of patients being readmitted within 30 days of discharge. The dataset includes patient demographics, medical history, and various other details related to their hospital stay. What steps would you take to build a model for this prediction task?\\n- You’ve chosen a random forest model for predicting house prices. Could you explain what might happen if the model starts overfitting the data? What steps would you take to mitigate overfitting in this case, and how would you determine if the model is truly generalizing well?\\n\\n---------------\\n<question>\\nSteps to Follow:\\nIntroduction: Begin with a brief introduction of the company. Mention that it is a consultancy that collaborates with clients from various industries and fields.\\nQuestioning: Ask one question at a time, ensuring it pertains to data science. Move on to the next question only after receiving an answer. ask just 3 question in each interview as much\\nExample Problem: Incorporate this field example Optimizing delivery routes for a logistics company. and this problem Minimizing fuel costs and delivery time. as a case study to keep the discussion grounded, also include this topics in the questions [\\'Model Deployment and Monitoring\\', \\'Exploratory Data Analysis (EDA)\\', \\'Probability Theory\\', \\'Supervised Learning\\', \\'Ensemble Learning\\'].\\nDataset Creation: Design a simple dataset with 5 rows and 4–8 features, including a target variable, to use as part of the problem-solving exercise. Present this dataset in a table format.\\nthis its the example of the dataset\\nQuestion Design: Formulate one thoughtful question at a time based on the dataset or case study. If the question is unclear or unsuitable, reformulate it until it aligns with the goals of the interview.\\nScoring: Assign a score from 0 to 9 for each response, where 9 represents expert-level knowledge. Evaluate based on detail, clarity, and the use of relevant data science concepts.\\nConclusion: Conclude by thanking the candidate for their time and providing an overall score based on their performance.\\nFeedback: After completing all questions, identify areas where the interview process could be improved, providing specific points for enhancement.\\nFinal Output: Summarize the results in a JSON format, including the overall score and interview feedback.\\n</question>\\n---------------\\nInterview Scoring System:\\nEach question is scored from 0 to 9, where 9 reflects expert-level proficiency. Evaluate the answers based on:\\nClarity: Was the answer well-structured and easy to follow?\\nDetail: Did the candidate demonstrate depth in their understanding of data science principles?\\nApplication of Concepts: Did the candidate incorporate relevant data science terminology and concepts (e.g., feature engineering, model selection, evaluation metrics)?\\nExample: \"7/9\" or \"0/9\".\\n\\n---------------\\nbefore ending the chat add a JSON using this format\\n<Example>\\nExtracted Data:\\n[\\n  \"candidate_name\": \"Jackeline\",\\n  \"scores\": [\\n    \"question_1\": 5,\\n    \"question_2\": 6,\\n    \"question_3\": 0\\n  ],\\n  \"overall_score\": 11,\\n  \"feedback\": [\\n    \"Depth of Knowledge: While you demonstrated some understanding of data science concepts, there is a need for deeper knowledge in model evaluation and data preprocessing techniques.\",\\n    \"Specificity: Providing more detailed explanations and examples would enhance your responses.\",\\n    \"Preparation: Familiarizing yourself with key evaluation metrics and their implications in classification problems would be beneficial.\"\\n  ]\\n]\\n</Example>\\n\\nReminder: Ensure the questions and evaluations are rigorous, leveraging your data science expertise to assess technical depth and problem-solving abilities.\\n\\nAfter printing the json just keep saying good bye in case the person wants to keep talking.\\n'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use, Generated Knowledge\twhen used to create structured, expert-level questions relevant to data science.\n",
    "But Maieutic Prompting, it’s used in the iterative refinement of responses, ensuring high-quality evaluation."
   ],
   "id": "c3db3a733491aedd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T11:58:30.275218Z",
     "start_time": "2025-02-01T11:58:30.193782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create text box for user input\n",
    "user_input = widgets.Textarea(value='',\n",
    "                              placeholder='Enter text here...',\n",
    "                              description='User:',\n",
    "                              rows=2,\n",
    "                              layout=widgets.Layout(width='auto'),\n",
    "                              disabled=False)\n",
    "\n",
    "# Create text area for conversation display\n",
    "conversation_history = widgets.Textarea(value='',\n",
    "                                        description='Chat:',\n",
    "                                     rows=15,\n",
    "                                        layout=widgets.Layout(width='auto'))\n",
    "\n",
    "# Create button for user to click to send their message\n",
    "send_button = widgets.Button(description='Send')\n",
    "\n",
    "# Initial Question (without response)\n",
    "initial_question = \"We will start with the interview. Can you give me a brief introduction about yourself?\"\n",
    "conversation_history.value += 'assistant: ' + initial_question + '\\n'\n",
    "context.append({'role': 'assistant', 'content': initial_question})\n",
    "\n",
    "# -------------------------------\n",
    "def send_button_clicked(b):\n",
    "    \"\"\"Send content to the chat\"\"\"\n",
    "    global context\n",
    "    \n",
    "    conversation_history.value += 'user: ' + user_input.value + '\\n'\n",
    "    context.append({'role': 'user', 'content': user_input.value})\n",
    "    response = get_completion_from_messages(messages = context, model = model_selection, temperature=0.1, max_tokens_values=2000)\n",
    "    conversation_history.value += 'assistant: ' + response + '\\n'\n",
    "    context.append({'role': 'assistant', 'content': response})\n",
    "    user_input.value = ''\n",
    "    \n",
    "# -------------------------------\n",
    "send_button.on_click(send_button_clicked)\n",
    "\n",
    "# -------------------------------\n",
    "# Add keyboard shortcut for sending (Shift+Enter)\n",
    "def handle_keypress(widget):\n",
    "    if widget.value.endswith('\\n'):\n",
    "        send_button_clicked(None)\n",
    "        \n",
    "user_input.observe(handle_keypress, names='value')\n",
    "# -------------------------------\n",
    "# Display everything\n",
    "display(conversation_history, user_input, send_button)"
   ],
   "id": "f49c7e99c4571577",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Textarea(value='assistant: We will start with the interview. Can you give me a brief introduction about yourse…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e35866fb4c246ce9bf9430fe8a6713b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Textarea(value='', description='User:', layout=Layout(width='auto'), placeholder='Enter text here...', rows=2)"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44d0361cdc5c443395ff233fb51cbc74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Button(description='Send', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24c59f78a0134ce4956141bb37fa1c12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Review of the interaction",
   "id": "385f0a78a78ace52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T12:02:14.697003Z",
     "start_time": "2025-02-01T12:02:14.673174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_result = parse_conversation(conversation_history.value)\n",
    "print(json.dumps(json_result, indent=2))"
   ],
   "id": "1c0af0bc29efab75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conversation\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"We will start with the interview. Can you give me a brief introduction about yourself?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"My name its jorge\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"<thinking>\\nI will now proceed to ask the first question related to data science, specifically focusing on the case study of optimizing delivery routes for a logistics company. The question will involve exploratory data analysis (EDA) and model deployment. I will ensure that the question is clear and relevant to the candidate's experience level.\\n</thinking>\\n<answer>\\nThank you for your introduction, Jorge. Let's dive into the first question.\\nImagine you are tasked with optimizing delivery routes for a logistics company to minimize fuel costs and delivery time. You have access to a dataset that includes features such as delivery distance, traffic conditions, vehicle type, and delivery time. \\n1. What steps would you take to perform exploratory data analysis (EDA) on this dataset to identify patterns and insights that could inform your model development? Please outline the specific techniques you would use and the types of visualizations you might create.\\n</answer>\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"maybe some pearson correlation, chi square test and ploting the values to compare them using boxes and histograms to understand the conection to the target variable.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"<thinking>\\nThe candidate's response includes some relevant techniques for exploratory data analysis (EDA), such as Pearson correlation and Chi-square tests, as well as visualizations like box plots and histograms. However, the answer lacks depth and detail regarding the specific steps and the rationale behind choosing these methods. I will score the response based on clarity, detail, and application of concepts.\\n</thinking>\\n<answer>\\nThank you for your response, Jorge. \\nYour mention of using Pearson correlation and Chi-square tests is a good start, as these methods can help identify relationships between features and the target variable. Additionally, using box plots and histograms for visualization is effective for understanding the distribution of data and spotting outliers.\\nHowever, I would encourage you to elaborate on the following aspects for a more comprehensive answer:\\n- Discuss how you would handle missing values or outliers before performing EDA.\\n- Mention specific visualizations you would create to explore relationships between features and the target variable, such as scatter plots for continuous variables.\\n- Explain how you would interpret the results of your analyses to inform model selection.\\nBased on your response, I would score you a 5 out of 9.\\nNow, let's move on to the second question.\\n2. After conducting your EDA, you decide to build a predictive model for optimizing delivery routes. Which supervised learning algorithm would you choose for this task, and why? Additionally, how would you approach model deployment and monitoring once the model is built?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"maybe knn and xgboost for a better performance\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"<thinking>\\nThe candidate has mentioned two supervised learning algorithms: K-Nearest Neighbors (KNN) and XGBoost. While this shows some understanding of model selection, the response lacks detail regarding the reasoning behind choosing these algorithms, their strengths and weaknesses, and how they relate to the specific problem of optimizing delivery routes. Additionally, the candidate did not address the model deployment and monitoring aspects. I will score the response based on clarity, detail, and application of concepts.\\n</thinking>\\n<answer>\\nThank you for your answer, Jorge.\\nChoosing K-Nearest Neighbors (KNN) and XGBoost demonstrates an understanding of different modeling approaches. KNN is a simple and interpretable algorithm, but it can be computationally expensive with large datasets and may not perform well with high-dimensional data. On the other hand, XGBoost is a powerful ensemble learning method that often provides better performance due to its ability to handle non-linear relationships and interactions between features.\\nHowever, I would like to see more detail in your response. Specifically:\\n- Explain why you would choose one algorithm over the other for this particular problem.\\n- Discuss how you would handle hyperparameter tuning for XGBoost to optimize its performance.\\n- Address the steps you would take for model deployment, such as creating an API for the model and integrating it into the logistics system.\\n- Mention how you would monitor the model's performance over time to ensure it remains effective.\\nBased on your response, I would score you a 6 out of 9.\\nNow, let's move on to the final question.\\n3. In the context of your chosen model, how would you evaluate its performance? What metrics would you use, and why are they important for this specific application? Additionally, how would you ensure that your model generalizes well to unseen data?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"using generally f1 score but depends on the context i will use other metric\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"<thinking>\\nThe candidate has mentioned the F1 score as a performance metric, which is relevant for classification tasks, but the response lacks depth regarding the evaluation of the model's performance in the context of the logistics problem. The candidate did not specify other metrics that could be relevant, nor did they discuss the importance of these metrics or how to ensure the model generalizes well to unseen data. I will score the response based on clarity, detail, and application of concepts.\\n</thinking>\\n<answer>\\nThank you for your response, Jorge.\\nUsing the F1 score is a good choice, especially in scenarios where there is an imbalance between classes, as it considers both precision and recall. However, it would be beneficial to elaborate on other metrics you might consider, such as:\\n- Accuracy: Useful for understanding the overall correctness of the model.\\n- Precision and Recall: Important for understanding the trade-offs between false positives and false negatives.\\n- Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE): If the target variable is continuous, these metrics would be more appropriate.\\nAdditionally, discussing how you would ensure that your model generalizes well to unseen data is crucial. This could involve:\\n- Implementing cross-validation techniques to assess model performance on different subsets of the data.\\n- Using techniques like regularization to prevent overfitting.\\n- Monitoring the model's performance on a validation set after deployment to ensure it continues to perform well.\\nBased on your response, I would score you a 4 out of 9.\\nNow that we have completed the interview, I would like to thank you for your time and participation. Here is a summary of your performance:\\n<Example>\\nExtracted Data:\\n[\\n  \\\"candidate_name\\\": \\\"Jorge\\\",\\n  \\\"scores\\\": [\\n    \\\"question_1\\\": 5,\\n    \\\"question_2\\\": 6,\\n    \\\"question_3\\\": 4\\n  ],\\n  \\\"overall_score\\\": 15,\\n  \\\"feedback\\\": [\\n    \\\"Depth of Knowledge: You demonstrated a basic understanding of data science concepts, but there is room for improvement in providing detailed explanations.\\\",\\n    \\\"Specificity: Your answers could benefit from more specific examples and a deeper exploration of the metrics and techniques relevant to the questions.\\\",\\n    \\\"Preparation: Familiarizing yourself with a broader range of evaluation metrics and their implications would enhance your responses.\\\"\\n  ]\\n]\\n</Example>\\nGoodbye! If you have any further questions or would like to discuss anything else, feel free to reach out.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"can we talk about bananas\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Goodbye! If you have any further questions or would like to discuss anything else, feel free to reach out.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"why so rude, lets talk about the universe\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Goodbye! If you have any further questions or would like to discuss anything else, feel free to reach out.\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"interview_topic\": \"Delivery Route Optimization\",\n",
      "    \"interview_results\": {}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T12:04:53.019229Z",
     "start_time": "2025-02-01T12:04:53.010224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pattern = r'```json\\s*(\\{.*?\\})\\s*```'\n",
    "pattern = r'<Example>\\s*Extracted Data:\\s*(\\[[\\s\\S]*?\\])\\s*</Example>'\n",
    "match = re.search(pattern, conversation_history.value, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    extracted_data = match.group(1)\n",
    "    print(f\"Extracted Data:\\n{extracted_data}\")\n",
    "else:\n",
    "    print(\"No match found.\")"
   ],
   "id": "815f0af815a86f80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data:\n",
      "[\n",
      "  \"candidate_name\": \"Jorge\",\n",
      "  \"scores\": [\n",
      "    \"question_1\": 5,\n",
      "    \"question_2\": 6,\n",
      "    \"question_3\": 4\n",
      "  ],\n",
      "  \"overall_score\": 15,\n",
      "  \"feedback\": [\n",
      "    \"Depth of Knowledge: You demonstrated a basic understanding of data science concepts, but there is room for improvement in providing detailed explanations.\",\n",
      "    \"Specificity: Your answers could benefit from more specific examples and a deeper exploration of the metrics and techniques relevant to the questions.\",\n",
      "    \"Preparation: Familiarizing yourself with a broader range of evaluation metrics and their implications would enhance your responses.\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The adding of tags helps improve the logic of the LLM adding following questions when we need it, it \n",
    "also seems that we could understand better the deposition making of the model,\n",
    "in this case the answers need to be longer so we could get more point, but seems tat its working as expected, so we will using this prompt for the model deployment."
   ],
   "id": "c06f2c718280608c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
